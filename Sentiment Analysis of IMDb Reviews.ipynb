{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgQ50lDrqyI2"
      },
      "source": [
        "## Import Libraries\n",
        "The libraries used in this homework are already preinstalled in Google Colab. If you are working locally, make sure to create a virtual environment and install the required libraries there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "nxLzIUEa1GZd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/halilibrahimumutcolak/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/halilibrahimumutcolak/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 265,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrC-jQzS-Pli"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "2ge7pX4yIW2j"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qWmgoMgr56hpCEjJe0XQ-K46-yMpFhVS\n",
            "To: /Users/halilibrahimumutcolak/Desktop/cs412hw2/imdb_dataset.csv\n",
            "100%|██████████| 66.2M/66.2M [00:03<00:00, 16.7MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'imdb_dataset.csv'"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Downloading data\n",
        "import gdown\n",
        "data_url = \"https://drive.google.com/uc?id=1qWmgoMgr56hpCEjJe0XQ-K46-yMpFhVS\"\n",
        "save_path = \"imdb_dataset.csv\"\n",
        "gdown.download(data_url, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "8EbbBMGUIrrr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"imdb_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LNWT1YU-Mx-"
      },
      "source": [
        "## **1. Preproces text data** (30 pts)\n",
        "In this section, you have to preprocess the text data in order to convert it to a numerical representation that can be passed to machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy5DxN7xCHPa"
      },
      "source": [
        "### 1.1 **Remove HTML**\n",
        "Since these reviews were scraped from the internet, some of them still have some HTML tags (like \\<a> or \\<br>). These tags do not have any semantic meaning. Therefore, we have to remove them to avoid having noise in our data. Otherwise, the model will assume that these are words that have meanings. Here is one example of HTML tags in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "3nEDD7NCC4bx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ]
          },
          "execution_count": 268,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[1, 'review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "3TOLq2eyJNKy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gp/xj9546954wjfm7yyl8jflm5r0000gn/T/ipykernel_26333/818014833.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove HTML\n",
        "# Function to remove HTML tags from a given text\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Apply the remove_html_tags function to the 'review' column\n",
        "df['review'] = df['review'].apply(remove_html_tags)\n",
        "\n",
        "df.loc[1, 'review']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJHkc3ZvG3NA"
      },
      "source": [
        "### 1.2 **Converting text to lowercase**\n",
        "\n",
        "In text analysis, we usually convert text to lowercase to avoid having a very large vocabulary. For instance, if we do not convert the text to lowercase, the model will treat \"apple\", \"Apple\", and \"APPLE\" as completely different words. This way the number of words that the model has to know (which we call vocabulary) becomes very large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "CSztuMchJL_Y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'a wonderful little production. the filming technique is very unassuming- very old-time-bbc fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. the actors are extremely well chosen- michael sheen not only \"has got all the polari\" but he has all the voices down pat too! you can truly see the seamless editing guided by the references to williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. a masterful production about one of the great master\\'s of comedy and his life. the realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. it plays on our knowledge and our senses, particularly with the scenes concerning orton and halliwell and the sets (particularly of their flat with halliwell\\'s murals decorating every surface) are terribly well done.'"
            ]
          },
          "execution_count": 270,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert to lower case\n",
        "\n",
        "# Function to remove HTML tags from a given text\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def convert_to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "# Apply the convert_to_lowercase function to the 'review' column\n",
        "df['review'] = df['review'].apply(convert_to_lowercase)\n",
        "\n",
        "df.loc[1, 'review']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wijyla-wDDlh"
      },
      "source": [
        "### 1.3 **Remove punctuation and special characters**\n",
        "Since our goal in this task is sentiment analysis, punctuations and special characters (like parantheses, percentage signs, etc.) do not change the sentiment of a movie review. Therefore we remove them to reduce the vocabulary size. One may argue that the exclamation (!) mark may affect the intensity of a sentiment. You have the choice to choose which punctuation marks to remove and which ones to keep. You have to remove some of them though to get the points for this part :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "HgkQeouQJK5I"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "# Remove punctuation and special characters\n",
        "def remove_punctuation(text):\n",
        "    # Define the custom set of punctuation marks to keep\n",
        "    custom_punctuation = ['!', '.', '?']  # Added question mark as an optional punctuation to retain\n",
        "\n",
        "    # Remove all punctuation except the custom set\n",
        "    translator = str.maketrans('', '', f'{string.punctuation.replace(\"\".join(custom_punctuation), \"\")}')\n",
        "    return text.translate(translator)\n",
        "\n",
        "df['review'] = df['review'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAmSnCegI4vG"
      },
      "source": [
        "### 1.4 **Tokenization**\n",
        "Now we want to convert each review into a sequence of words. This process is called tokenization. **Use `word_tokenize` from the `nltk.tokenize` module to tokenize each review**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "xJ-359NZJIq4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sentence: ['Hello', '!', 'this', 'is', 'a', 'test']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "test = 'Hello! this is a test'\n",
        "print(f\"Tokenized sentence: {word_tokenize(test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "YcUYM32DbuK5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review\n",
            "0  [one, of, the, other, reviewers, has, mentione...\n",
            "1  [a, wonderful, little, production, the, filmin...\n",
            "2  [i, thought, this, was, a, wonderful, way, to,...\n",
            "3  [basically, theres, a, family, where, a, littl...\n",
            "4  [petter, matteis, love, in, the, time, of, mon...\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the review texts\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['review'] = df['review'].apply(tokenize_text)\n",
        "\n",
        "print(df[['review']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dogh5QLQJcSx"
      },
      "source": [
        "### 1.5 **Removing stop words**\n",
        "\n",
        "\n",
        "We also need to remove the stopwords (is, the, you, etc.) since they do not add to the meaning. Why? To reduce the vocabulary size 😃. **You can get a list of all stopwords from the nltk library**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "TufKm5YsL8hz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review\n",
            "0  [one, reviewers, mentioned, watching, 1, oz, e...\n",
            "1  [wonderful, little, production, filming, techn...\n",
            "2  [thought, wonderful, way, spend, time, hot, su...\n",
            "3  [basically, theres, family, little, boy, jake,...\n",
            "4  [petter, matteis, love, time, money, visually,...\n"
          ]
        }
      ],
      "source": [
        "# Remove stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "df['review'] = df['review'].apply(remove_stopwords)\n",
        "\n",
        "print(df[['review']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqvJ6QUVON1B"
      },
      "source": [
        "### 1.6 **Stemming**\n",
        "\n",
        "Apply stemming using one of the `SnowballStemmer` stemmers in the nltk library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "rrbB6dtK-gZJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    [one, review, mention, watch, 1, oz, episod, y...\n",
            "1    [wonder, littl, product, film, techniqu, unass...\n",
            "2    [thought, wonder, way, spend, time, hot, summe...\n",
            "3    [basic, there, famili, littl, boy, jake, think...\n",
            "4    [petter, mattei, love, time, money, visual, st...\n",
            "Name: review, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Apply Stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Function to apply stemming using SnowballStemmer\n",
        "def apply_stemming(tokens):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Apply the apply_stemming function to the 'filtered_review' column\n",
        "df['review'] = df['review'].apply(apply_stemming)\n",
        "\n",
        "# Display the DataFrame with the stemmed reviews\n",
        "print(df[\"review\"].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nnGZFTqdbna"
      },
      "source": [
        "### 1.7 **Joining review text**\n",
        "Now that we have removed the stopwords and applied stemming on each word, we need to convert each review from a list of word to a space-separated string. The reason we do this is that the functions that are used to extract numerical representations from text expect the input to be a single string not a list of words.\n",
        "\n",
        "**Convert each review from a list of words to a space-separated string.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "oPfZK9fFeOXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    one review mention watch 1 oz episod youll hoo...\n",
            "1    wonder littl product film techniqu unassum old...\n",
            "2    thought wonder way spend time hot summer weeke...\n",
            "3    basic there famili littl boy jake think there ...\n",
            "4    petter mattei love time money visual stun film...\n",
            "Name: review, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Your solution\n",
        "# Convert each review from a list of words to a space-separated string\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Display the DataFrame with the converted reviews\n",
        "print(df['review'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOTAoMoFq706"
      },
      "source": [
        "## 2. **Buidling Simple NN Models** (70 Pts)\n",
        "In this section, you will build neural network models to predict the sentiment of each review. You will build three models, where each of the models uses a different numerical representation of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "XlluZFZ7YmYF"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries here\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iada5kfkYtPr"
      },
      "source": [
        "### 2.1 **Data preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y4MmQQ3WRSM"
      },
      "source": [
        "#### 2.1.1 **Encoding labels**\n",
        "Encode the labels (\"positive\" and \"negative\") as 0 for \"negative\" and 1 for \"positive\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "ePwco1nbW7o8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review  sentiment\n",
            "0  one review mention watch 1 oz episod youll hoo...          1\n",
            "1  wonder littl product film techniqu unassum old...          1\n",
            "2  thought wonder way spend time hot summer weeke...          1\n",
            "3  basic there famili littl boy jake think there ...          0\n",
            "4  petter mattei love time money visual stun film...          1\n"
          ]
        }
      ],
      "source": [
        "# Encode labels\n",
        "# Encode labels\n",
        "df['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "# Display the DataFrame with encoded labels\n",
        "print(df[['review', 'sentiment']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK92SdrYWyrs"
      },
      "source": [
        "#### 2.1.2 **Splitting data**\n",
        "Split your data into train, validation, and test sets. You should use 80% of your data for training and 20% for testing. You should also use 20% of your training data for validation. **Set random_state to 42 to ensure consistent results.**\n",
        "\n",
        "Hint: You can pass parameters in Keras to use some of your data for validation while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "jFskqoM2XV-I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: (32000,)\n",
            "Validation data shape: (8000,)\n",
            "Test data shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df['review'],  # Features (review text)\n",
        "    df['sentiment'],  # Labels (sentiment)\n",
        "    test_size=0.2,  # 20% for testing\n",
        "    random_state=42  # Set random_state for reproducibility\n",
        ")\n",
        "\n",
        "# Split the training data into train and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    test_size=0.2,  # 20% for validation\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Display the shapes of the resulting sets\n",
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUEouGNLZiat"
      },
      "source": [
        "#### **Exploring vocabulary**\n",
        "Create a dictionary that maps each word in your training data to the number of times it occurs in the training data. The dictionary should have the following format:\n",
        "\n",
        "```\n",
        "vocab_count = {\n",
        "  \"word1\": 1,\n",
        "  \"word2\": 120,\n",
        "  ...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "xrBcdP3maD53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fault': 402, 'actor': 8403, 'put': 3975, 'great': 11682, 'perform': 6687}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Tokenize the training data\n",
        "train_tokens = [word_tokenize(text) for text in train_data]\n",
        "\n",
        "# Flatten the list of tokens\n",
        "flat_tokens = [token for sublist in train_tokens for token in sublist]\n",
        "\n",
        "# Create a dictionary for vocabulary count in training data\n",
        "vocab_count = Counter(flat_tokens)\n",
        "\n",
        "# Display the first few entries of the vocabulary count dictionary\n",
        "print({k: vocab_count[k] for k in list(vocab_count)[:5]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "_20IICeefsux"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 134185\n",
            "Top 15 frequent words: movi, film, one, like, time, good, make, see, charact, get, watch, even, stori, would, realli\n",
            "Least frequent 15 words: itbuy, girlssur, handheldcamera, soundwis, wisethan, eurohous, hardrockmi, halfsh, liethi, awaymild, heralthough, failbeyond, occurredestevez, itrecommend, ushad\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run this cell to explore the vocabulary\n",
        "import seaborn as sns\n",
        "count_arr = np.array(list(vocab_count.values()))\n",
        "vocab = np.array(list(vocab_count.keys()))\n",
        "\n",
        "# Sorting by frequency\n",
        "sort_idx = np.argsort(count_arr)[::-1]\n",
        "vocab = vocab[sort_idx]\n",
        "count_arr = count_arr[sort_idx]\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Top 15 frequent words: {', '.join(vocab[:15])}\")\n",
        "print(f\"Least frequent 15 words: {', '.join(vocab[-15:])}\")\n",
        "print()\n",
        "\n",
        "ax = sns.histplot(count_arr, log_scale=True, bins=100)\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Number of word occurrences')\n",
        "ax.set_title(\"Histogram of vocabulary frequencies in training data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ArllJNd_RL8"
      },
      "source": [
        "### 2.2 **Bag of Words (BoW) model**\n",
        "Create a simple NN model that utilizes the Bag of Words representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlAe5IAzY7i3"
      },
      "source": [
        "#### 2.2.1 **Generate Bag of Words (BoW) representation**\n",
        "\n",
        "You can use the `CountVectorizer` class from `sklearn` to do this. Since your data is already preprocessed, set the `preprocessor` argument of `CountVectorizer` to pass a function that does nothing (`lambda x: x`). This way, `CountVectorizer` will not do any preprocessing on your data.\n",
        "\n",
        "Choose a reasonable value for the `max_features` argument. This argument determines how many words will be included in your vocabulary. For example, if your total vocabulary count is 100,000 and you set `max_features = 2,000`, `CountVectorizer` will only consider the **most frequent** 2,000 words in your vocabulary. The rest of the vocabulary will be ignored.\n",
        "\n",
        "* [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "cvIioVluY4TA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train_bow: (32000, 5000)\n",
            "Shape of test_bow: (10000, 5000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Choose a value for max_features\n",
        "max_features = 5000  # Adjust as needed\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=max_features, preprocessor=lambda x: x)\n",
        "\n",
        "# Fit the vectorizer to your train data\n",
        "train_bow = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Transform the train and test data\n",
        "test_bow = vectorizer.transform(test_data)\n",
        "\n",
        "# Display the shape of the resulting BoW matrices\n",
        "print(f\"Shape of train_bow: {train_bow.shape}\")\n",
        "print(f\"Shape of test_bow: {test_bow.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "2vlTK6pl0IXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32000, 5000)\n"
          ]
        }
      ],
      "source": [
        "# The shape of the train data BoW representation should be\n",
        "# (Number of data points in training data x max_features)\n",
        "print(train_bow.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnVbn50ZTs-"
      },
      "source": [
        "#### 2.2.2 **Train a neural network on BoW data**\n",
        "Use a small neural network to predict the sentiment of reviews from the BoW representation of the reviews. Use the keras library for this task.\n",
        "\n",
        "**Notes**:\n",
        "* Your neural network should have **maximum two hidden layers**.\n",
        "* Train for **10 epochs or less**\n",
        "* Choose any method you find suitable to **avoid overfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "Ut5c1ivakTvc"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "X5H-Wsu60jXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_87 (Dense)            (None, 64)                320064    \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322177 (1.23 MB)\n",
            "Trainable params: 322177 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3498 - accuracy: 0.8560 - val_loss: 0.3014 - val_accuracy: 0.8786\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2081 - accuracy: 0.9176 - val_loss: 0.3163 - val_accuracy: 0.8702\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.9605 - val_loss: 0.4260 - val_accuracy: 0.8666\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0361 - accuracy: 0.9892 - val_loss: 0.6102 - val_accuracy: 0.8642\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.7700 - val_accuracy: 0.8667\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.9367 - val_accuracy: 0.8627\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.9957 - val_accuracy: 0.8633\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0173 - accuracy: 0.9948 - val_loss: 0.9866 - val_accuracy: 0.8583\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 1.0211 - val_accuracy: 0.8569\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 1.1638 - val_accuracy: 0.8619\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=max_features))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Show model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "# Set validation_split=0.20 to use 20% of the train data for validation\n",
        "history = model.fit(train_bow.toarray(), train_labels, epochs=10, batch_size=32, validation_split=0.20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bQfHwju1vx_"
      },
      "source": [
        "#### 2.2.3 **Evaluate on test dataset**\n",
        "After choosing the suitable model hyperparameters and training a model in the previous step, evaluate your model on the test set. Choose the metrics you find suitable for evauluating this model.\n",
        "\n",
        "**Note**: Evaluation on the test dataset should only be done once. Updating your model iteratively until you get a good performance on the test dataset is not a good practice and will result in overfitting on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "L_6rzD-y1qXb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 562us/step - loss: 1.1054 - accuracy: 0.8631\n",
            "Test Loss: 1.1054\n",
            "Test Accuracy: 0.8631\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_bow.toarray(), test_labels)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFYph9dm_VOr"
      },
      "source": [
        "### 2.3 **TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUn_mGiMZLuq"
      },
      "source": [
        "Create a simple NN model that utilizes the TF-IDF representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9pwr-uZ3Ik1"
      },
      "source": [
        "#### 2.3.1 **Generate TF-IDF representation**\n",
        "\n",
        "You can use the `TfidfVectorizer` class from `sklearn` to do this. Since your data is already preprocessed, set the `preprocessor` argument of `TfidfVectorizer` to pass a function that does nothing (`lambda x: x`). This way, `TfidfVectorizer` will not do any preprocessing on your data.\n",
        "\n",
        "Choose a reasonable value for the `max_features` argument.\n",
        "\n",
        "* [TfidfVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "QEQa7Y4I_W_8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Choose a value for max_features\n",
        "max_features = 5000  # Adjust as needed\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, preprocessor=lambda x: x)\n",
        "\n",
        "# Fit the vectorizer to your train data\n",
        "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Transform the train and test data\n",
        "test_tfidf = tfidf_vectorizer.transform(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TU6aX5N40Mx"
      },
      "source": [
        "#### 2.3.2 **Train a neural network on TF-IDF data**\n",
        "Use a small neural network to predict the sentiment of reviews from the TF-IDF representation of the reviews. Use the keras library for this task. **You can use the same neural network structure form the BoW model.**\n",
        "\n",
        "**Notes**:\n",
        "* Your neural network should have **maximum two hidden layers**.\n",
        "* Train for **10 epochs or less**\n",
        "* Choose any method you find suitable to **avoid overfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "WfVf7osX49KU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_tfidf shape: (32000, 5000)\n",
            "train_labels shape: (32000,)\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_90 (Dense)            (None, 64)                320064    \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322177 (1.23 MB)\n",
            "Trainable params: 322177 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3376 - accuracy: 0.8628 - val_loss: 0.2875 - val_accuracy: 0.8800\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2281 - accuracy: 0.9102 - val_loss: 0.3054 - val_accuracy: 0.8758\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1930 - accuracy: 0.9258 - val_loss: 0.3223 - val_accuracy: 0.8706\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1445 - accuracy: 0.9467 - val_loss: 0.3620 - val_accuracy: 0.8581\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0813 - accuracy: 0.9741 - val_loss: 0.4619 - val_accuracy: 0.8595\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.5606 - val_accuracy: 0.8572\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 0.7385 - val_accuracy: 0.8561\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.8827 - val_accuracy: 0.8545\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.0027 - val_accuracy: 0.8536\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.7628e-04 - accuracy: 1.0000 - val_loss: 1.1347 - val_accuracy: 0.8558\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Assuming 'train_tfidf' is your input TF-IDF data and 'train_labels' are your labels\n",
        "print(\"train_tfidf shape:\", train_tfidf.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "input_shape = train_tfidf.shape[1]\n",
        "\n",
        "# Build the Model\n",
        "model_tfidf = Sequential()\n",
        "model_tfidf.add(Dense(64, activation='relu', input_shape=(input_shape,)))  # First hidden layer\n",
        "model_tfidf.add(Dense(32, activation='relu'))  # Second hidden layer\n",
        "model_tfidf.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the Model\n",
        "model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Show Model Summary\n",
        "model_tfidf.summary()\n",
        "\n",
        "# Train the Model\n",
        "# Assuming train_labels has the correct shape (32000,)\n",
        "history_tfidf = model_tfidf.fit(train_tfidf.toarray(), train_labels, epochs=10, validation_split=0.20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hPry9cx5FFb"
      },
      "source": [
        "#### 2.2.3 **Evaluate on test dataset**\n",
        "After choosing the suitable model hyperparameters and training a model in the previous step, evaluate your model on the test set. Choose the metrics you find suitable for evauluating this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "C3W9Dh7xIip3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 496us/step - loss: 1.0624 - accuracy: 0.8627\n",
            "Test Loss: 1.0624\n",
            "Test Accuracy: 0.8627\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'test_tfidf' is your input TF-IDF data for the test set and 'test_labels' are the corresponding labels\n",
        "evaluation_results = model_tfidf.evaluate(test_tfidf.toarray(), test_labels)\n",
        "\n",
        "# Extract the loss and accuracy from the results\n",
        "test_loss, test_accuracy = evaluation_results\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZj46dBC_XnP"
      },
      "source": [
        "### 2.4 **Word2Vec**\n",
        "In this section, you will use Word2Vec representations to train a simple neural network to predict the sentiments of reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX6AhwZB5ltk"
      },
      "source": [
        "#### 2.4.1 **Preliminary About Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "zxil2TcVCJEm"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxehiNoc6EPr"
      },
      "source": [
        "The following cell will load a pretrained Word2Vec model. This is a model trained on a large corpus of data to produce meaningful representations of words as vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsjavca9_auI",
        "outputId": "5ff19d09-59e8-4021-b7b8-b4aa7dcebdc6"
      },
      "outputs": [],
      "source": [
        "# Loading pretrained Word2Vec model (Word embeddings)\n",
        "model_name = \"glove-wiki-gigaword-100\"\n",
        "word_to_vec = gensim.downloader.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "fNxPVPUz7G7E"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.30817  ,  0.30938  ,  0.52803  , -0.92543  , -0.73671  ,\n",
              "        0.63475  ,  0.44197  ,  0.10262  , -0.09142  , -0.56607  ,\n",
              "       -0.5327   ,  0.2013   ,  0.7704   , -0.13983  ,  0.13727  ,\n",
              "        1.1128   ,  0.89301  , -0.17869  , -0.0019722,  0.57289  ,\n",
              "        0.59479  ,  0.50428  , -0.28991  , -1.3491   ,  0.42756  ,\n",
              "        1.2748   , -1.1613   , -0.41084  ,  0.042804 ,  0.54866  ,\n",
              "        0.18897  ,  0.3759   ,  0.58035  ,  0.66975  ,  0.81156  ,\n",
              "        0.93864  , -0.51005  , -0.070079 ,  0.82819  , -0.35346  ,\n",
              "        0.21086  , -0.24412  , -0.16554  , -0.78358  , -0.48482  ,\n",
              "        0.38968  , -0.86356  , -0.016391 ,  0.31984  , -0.49246  ,\n",
              "       -0.069363 ,  0.018869 , -0.098286 ,  1.3126   , -0.12116  ,\n",
              "       -1.2399   , -0.091429 ,  0.35294  ,  0.64645  ,  0.089642 ,\n",
              "        0.70294  ,  1.1244   ,  0.38639  ,  0.52084  ,  0.98787  ,\n",
              "        0.79952  , -0.34625  ,  0.14095  ,  0.80167  ,  0.20987  ,\n",
              "       -0.86007  , -0.15308  ,  0.074523 ,  0.40816  ,  0.019208 ,\n",
              "        0.51587  , -0.34428  , -0.24525  , -0.77984  ,  0.27425  ,\n",
              "        0.22418  ,  0.20164  ,  0.017431 , -0.014697 , -1.0235   ,\n",
              "       -0.39695  , -0.0056188,  0.30569  ,  0.31748  ,  0.021404 ,\n",
              "        0.11837  , -0.11319  ,  0.42456  ,  0.53405  , -0.16717  ,\n",
              "       -0.27185  , -0.6255   ,  0.12883  ,  0.62529  , -0.52086  ],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting the representation of a word\n",
        "dog = word_to_vec[\"dog\"]\n",
        "dog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2Q_Ave6U1F"
      },
      "source": [
        "These representations are meaningful in a sense that words that have similar meanings are closer to each other. In general, when dealing with vector representations, we use **cosine similarity** to see how similar two vectors are. it returns a value between -1 and +1, the higher the value the more similar two vectors are.\n",
        "\n",
        "Notice in the example below that **car** is more similar to **vehicle** than **cat**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "lNkRtwBr6xjR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between car and cat: [[0.31097826]]\n",
            "Similarity between car and vehicle: [[0.86308384]]\n"
          ]
        }
      ],
      "source": [
        "from  sklearn.metrics.pairwise import cosine_similarity\n",
        "car = word_to_vec[\"car\"].reshape(1, -1) # Reshaping because cosine_similarity function requires a 2D array\n",
        "cat = word_to_vec[\"cat\"].reshape(1, -1)\n",
        "vehicle = word_to_vec[\"vehicle\"].reshape(1, -1)\n",
        "\n",
        "print(f\"Similarity between car and cat: {cosine_similarity(car, cat)}\")\n",
        "print(f\"Similarity between car and vehicle: {cosine_similarity(car, vehicle)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLS-06oTBMY-"
      },
      "source": [
        "#### 2.4.2 **Use pretrained Word2Vec model**\n",
        "First, you will try to use the pretrained Word2Vec model to generate representations for each review and use the representation to train a neural network.\n",
        "\n",
        "Remember that we are trying to classify movie reviews, which have can have any number of words. Word2Vec returns a vector representation for each word. Therefore, you have to find a way to create a **fixed-sized representation** for the whole review. Fixed-size means that the vector representation of each review should have the same size regardless of the length of the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "7RO53y0h4F8b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize(review, word_to_vec):\n",
        "    # Split the review into individual words\n",
        "    words = review.split()\n",
        "\n",
        "    # Initialize an array to store word vectors\n",
        "    word_vectors = []\n",
        "\n",
        "    # Iterate over words and add their vectors to the array\n",
        "    for word in words:\n",
        "        if word in word_to_vec:\n",
        "            word_vectors.append(word_to_vec[word])\n",
        "\n",
        "    # Check if there are any valid word vectors\n",
        "    if word_vectors:\n",
        "        # Average the word vectors to create a fixed-sized representation\n",
        "        review_vector = np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        # If no valid word vectors, return a zero vector\n",
        "        review_vector = np.zeros_like(word_to_vec[\"word\"])  # Assuming \"word\" is in the vocabulary\n",
        "\n",
        "    return review_vector\n",
        "\n",
        "# Apply the vectorize function to convert train and test reviews to vector representations\n",
        "train_vectors = np.array([vectorize(review, word_to_vec) for review in train_data])\n",
        "test_vectors = np.array([vectorize(review, word_to_vec) for review in test_data])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6nG9NJ8HzgW"
      },
      "source": [
        "Train a neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "fbLkFZXlIYYu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_vectors shape: (32000, 100)\n",
            "train_labels shape: (32000,)\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_93 (Dense)            (None, 64)                6464      \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8577 (33.50 KB)\n",
            "Trainable params: 8577 (33.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "800/800 [==============================] - 1s 564us/step - loss: 0.5485 - accuracy: 0.7252 - val_loss: 0.5069 - val_accuracy: 0.7564\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 0s 495us/step - loss: 0.5069 - accuracy: 0.7550 - val_loss: 0.5077 - val_accuracy: 0.7498\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 0s 498us/step - loss: 0.5019 - accuracy: 0.7556 - val_loss: 0.4993 - val_accuracy: 0.7564\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 0s 495us/step - loss: 0.4946 - accuracy: 0.7599 - val_loss: 0.4934 - val_accuracy: 0.7658\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 0s 498us/step - loss: 0.4904 - accuracy: 0.7653 - val_loss: 0.4898 - val_accuracy: 0.7659\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 0s 494us/step - loss: 0.4859 - accuracy: 0.7670 - val_loss: 0.4890 - val_accuracy: 0.7631\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 0s 498us/step - loss: 0.4829 - accuracy: 0.7701 - val_loss: 0.4896 - val_accuracy: 0.7644\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 0s 502us/step - loss: 0.4796 - accuracy: 0.7705 - val_loss: 0.4937 - val_accuracy: 0.7619\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 1s 659us/step - loss: 0.4770 - accuracy: 0.7721 - val_loss: 0.4889 - val_accuracy: 0.7684\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 0s 522us/step - loss: 0.4726 - accuracy: 0.7725 - val_loss: 0.4856 - val_accuracy: 0.7664\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Assuming 'train_vectors' is your input Word2Vec data and 'train_labels' are your labels\n",
        "print(\"train_vectors shape:\", train_vectors.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "input_shape = train_vectors.shape[1]\n",
        "\n",
        "# Build the Model\n",
        "model_word2vec = Sequential()\n",
        "model_word2vec.add(Dense(64, activation='relu', input_shape=(input_shape,)))  # First hidden layer\n",
        "model_word2vec.add(Dense(32, activation='relu'))  # Second hidden layer\n",
        "model_word2vec.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the Model\n",
        "model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Show Model Summary\n",
        "model_word2vec.summary()\n",
        "\n",
        "# Train the Model\n",
        "history_word2vec = model_word2vec.fit(train_vectors, train_labels, epochs=10, validation_split=0.20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj8W0JH0IZyM"
      },
      "source": [
        "Evaluate the model on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "Pa0a8PcdIdnK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 346us/step - loss: 0.4836 - accuracy: 0.7676\n",
            "Test Loss: 0.4836\n",
            "Test Accuracy: 0.7676\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Model on the test set\n",
        "test_loss, test_accuracy = model_word2vec.evaluate(test_vectors, test_labels)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIdPN7jwIuB_"
      },
      "source": [
        "#### 2.4.3 **Train your own Word2Vec model**\n",
        "In cases where we have enough data, training a new Word2Vec model may provide better results than using a pretrained one. Here, you will train your own Word2Vec model using the training data and then use it to create vector representations for the reviews.\n",
        "\n",
        "You can change the hyperparameters to train your Word2Vec model:\n",
        "* **vector_size**: The length of the vector that will be used to represent words.\n",
        "* **window**: The window sized used in the Word2Vec training\n",
        "* **min_count**: Minimum number of occurunces for a word to be included in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "HbyDteDyI6wM"
      },
      "outputs": [],
      "source": [
        "# Converting sentences to list of words\n",
        "sentences = [sentence.split() for sentence in train_data]\n",
        "\n",
        "# Training model\n",
        "vector_size = 500\n",
        "window = 5\n",
        "min_count = 5\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRtjznn9Jd_C"
      },
      "source": [
        "Apply the same function you created above to create vector representations for the reviews from your trained Word2Vec model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "WZXc1G6HJ1lz"
      },
      "outputs": [],
      "source": [
        "def vectorize(review, model):\n",
        "    # Get word vectors for each word in the review\n",
        "    word_vectors = [model.wv[word] for word in review.split() if word in model.wv.key_to_index]\n",
        "\n",
        "    # If no word vectors found, return a zero vector\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # Average the word vectors to get a fixed-sized vector for the review\n",
        "    review_vector = np.mean(word_vectors, axis=0)\n",
        "    return review_vector\n",
        "\n",
        "# Applying the function to convert train and test reviews to vector representations\n",
        "train_vectors = np.array([vectorize(review, w2v_model) for review in train_data])\n",
        "test_vectors = np.array([vectorize(review, w2v_model) for review in test_data])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4bnRmZKXO7q"
      },
      "source": [
        "Train a neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "nHxZvwcQXaXr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_96 (Dense)            (None, 64)                32064     \n",
            "                                                                 \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " dense_97 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34177 (133.50 KB)\n",
            "Trainable params: 34177 (133.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 1s 672us/step - loss: 0.3760 - accuracy: 0.8361 - val_loss: 0.3934 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 0s 612us/step - loss: 0.3478 - accuracy: 0.8493 - val_loss: 0.3446 - val_accuracy: 0.8489\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 0s 582us/step - loss: 0.3391 - accuracy: 0.8551 - val_loss: 0.3396 - val_accuracy: 0.8536\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 0s 560us/step - loss: 0.3335 - accuracy: 0.8556 - val_loss: 0.3496 - val_accuracy: 0.8475\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 0s 580us/step - loss: 0.3295 - accuracy: 0.8583 - val_loss: 0.3470 - val_accuracy: 0.8514\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 0s 611us/step - loss: 0.3253 - accuracy: 0.8590 - val_loss: 0.3410 - val_accuracy: 0.8519\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 0s 594us/step - loss: 0.3201 - accuracy: 0.8621 - val_loss: 0.3438 - val_accuracy: 0.8492\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 1s 633us/step - loss: 0.3186 - accuracy: 0.8637 - val_loss: 0.3627 - val_accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 0s 598us/step - loss: 0.3135 - accuracy: 0.8650 - val_loss: 0.3328 - val_accuracy: 0.8573\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 1s 634us/step - loss: 0.3113 - accuracy: 0.8677 - val_loss: 0.3407 - val_accuracy: 0.8552\n"
          ]
        }
      ],
      "source": [
        "# Build the neural network model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model_word2vec = Sequential()\n",
        "model_word2vec.add(Dense(64, activation='relu', input_shape=(vector_size,)))\n",
        "model_word2vec.add(Dense(32, activation='relu'))\n",
        "model_word2vec.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Show model summary\n",
        "model_word2vec.summary()\n",
        "\n",
        "# Train the model\n",
        "history_word2vec = model_word2vec.fit(train_vectors, train_labels, epochs=10, validation_split=0.20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86rLvA85XeoI"
      },
      "source": [
        "Evaluate the model on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "EwANruikXedY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 369us/step - loss: 0.3334 - accuracy: 0.8535\n",
            "Test Loss: 0.3334\n",
            "Test Accuracy: 0.8535\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Model# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model_word2vec.evaluate(test_vectors, test_labels)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
